---
title: "Home Credit Modeling Notebook"
author: "Joonas Tahvanainen"
date: "2024-10-30"
output:
  html_document:
    highlight: espresso
    number_sections: no
    toc: yes
  editor_options:
    chunk_output_type: console
---



```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(skimr)
library(summarytools)
library(janitor)
library(caret)
library(kernlab)
library(rminer) 
library(randomForest)
library(xgboost)
library(pROC) 
```



``` {r, message=FALSE, results='hide'}

application_train <- read_csv("application_train.csv")
application_test <- read_csv("application_test.csv")
bureau <- read_csv("bureau.csv")
home_credit_columns_description <- read_csv("HomeCredit_columns_description.csv")

```

# Data preparation


## Application Train clean up
First, I'm going to follow the same clean-up process as I did in my EDA notebook. This includes:
- Removing all the unnecessary columns(mostly all variables representing the applicant's living situation)
- Removing n/a values and converting negative values to positive for increased clarity
- Combining all credit score variables into one average credit score
- Adding credit profile flags based on existing credit information  
``` {r}

# Factoring all character variables
at_clean <- application_train %>%
  mutate(across(where(is.character), as.factor))

#Factoring all 'flag' varaibles
at_clean <- at_clean %>%
  mutate(across(matches("flag", ignore.case = TRUE), as.factor))

# Factoring all binary numeric variables
at_clean <- at_clean %>%
  mutate(across(c(REG_REGION_NOT_LIVE_REGION,
                  REG_REGION_NOT_WORK_REGION,
                  LIVE_REGION_NOT_WORK_REGION,
                  REG_CITY_NOT_LIVE_CITY,
                  REG_CITY_NOT_WORK_CITY,
                  LIVE_CITY_NOT_WORK_CITY,
                  TARGET), as.factor))

# Converting column names to lowercase
at_clean <- at_clean %>% clean_names()

# Defining all living situation variables that are unnecessary for modeling
living_situation_vars <- c(
  "apartments_avg", "basementarea_avg", "years_beginexpluatation_avg", 
  "years_build_avg", "commonarea_avg", "elevators_avg", 
  "entrances_avg", "floorsmax_avg", "floorsmin_avg", 
  "landarea_avg", "livingapartments_avg", "livingarea_avg", 
  "nonlivingapartments_avg", "nonlivingarea_avg", "apartments_mode", 
  "basementarea_mode", "years_beginexpluatation_mode", "years_build_mode", 
  "commonarea_mode", "elevators_mode", "entrances_mode", 
  "floorsmax_mode", "floorsmin_mode", "landarea_mode", 
  "livingapartments_mode", "livingarea_mode", "nonlivingapartments_mode", 
  "nonlivingarea_mode", "apartments_medi", "basementarea_medi", 
  "years_beginexpluatation_medi", "years_build_medi", "commonarea_medi", 
  "elevators_medi", "entrances_medi", "floorsmax_medi", 
  "floorsmin_medi", "landarea_medi", "livingapartments_medi", 
  "livingarea_medi", "nonlivingapartments_medi", "nonlivingarea_medi",  "totalarea_mode"
)

# Removing all living situation variables and a few others not defined before
at_clean <- at_clean %>%
  select(-all_of(living_situation_vars), 
         -fondkapremont_mode, 
         -housetype_mode, 
         -wallsmaterial_mode, 
         -emergencystate_mode)

# Fixing the issues with days employed variable
at_clean <- at_clean %>%
  mutate(days_employed = ifelse(days_employed > 0, 0, days_employed))

at_clean <- at_clean %>%
  mutate(days_employed = abs(days_employed))

# Simplifying the Occupation type variable
at_clean <- at_clean %>%
  mutate(occupation_type = case_when(
    is.na(occupation_type) & days_employed > 0 ~ 'Not listed',
    is.na(occupation_type) & days_employed == 0 ~ 'Unemployed',
    TRUE ~ occupation_type  # Keep original value if not NA
  )) %>% mutate(occupation_type = factor(occupation_type))

# Removing all rows where name_type_suite is n/a
at_clean <- at_clean %>%
  filter(!is.na(name_type_suite))

# Combining credit scores to an average credit score
at_clean <- at_clean %>%
  mutate(avg_credit_score = rowMeans(
    select(., ext_source_1, ext_source_2, ext_source_3), 
    na.rm = TRUE
  )) %>% 
  mutate(avg_credit_score = ifelse(is.na(avg_credit_score), 0, avg_credit_score))

# Creating credit flags based on how many bureau credit scores are available
at_clean <- at_clean %>%
  mutate(
    limited_credit_flag = case_when(
      rowSums(!is.na(select(., ext_source_1, ext_source_2, ext_source_3))) %in% 1:2 ~ 1,
      TRUE ~ 0
    ),
    no_credit_flag = case_when(
      rowSums(is.na(select(., ext_source_1, ext_source_2, ext_source_3))) == 3 ~ 1,
      TRUE ~ 0
    ),
    full_credit_flag = case_when(
      rowSums(!is.na(select(., ext_source_1, ext_source_2, ext_source_3))) == 3 ~ 1,
      TRUE ~ 0
    )
  ) %>%
  mutate(
    limited_credit_flag = factor(limited_credit_flag),
    no_credit_flag = factor(no_credit_flag),
    full_credit_flag = factor(full_credit_flag)
  ) %>%
  select(-ext_source_1, -ext_source_2, -ext_source_3)

# Simplifying the own car age variable 
at_clean <- at_clean %>%
  mutate(own_car_age = case_when(
    is.na(own_car_age) ~ 'No car',
    own_car_age >= 10 ~ '10+ years',
    own_car_age < 10 ~ 'Less than 10 years'
  )) %>%
  mutate(own_car_age = as.factor(own_car_age))

# Replacing n/a's with 0
at_clean <- at_clean %>%
  mutate(
    amt_req_credit_bureau_hour = replace_na(amt_req_credit_bureau_hour, 0),
    amt_req_credit_bureau_day = replace_na(amt_req_credit_bureau_day, 0),
    amt_req_credit_bureau_week = replace_na(amt_req_credit_bureau_week, 0),
    amt_req_credit_bureau_mon = replace_na(amt_req_credit_bureau_mon, 0),
    amt_req_credit_bureau_qrt = replace_na(amt_req_credit_bureau_qrt, 0),
    amt_req_credit_bureau_year = replace_na(amt_req_credit_bureau_year, 0)
  )

# Removing all rows where the following variables are n/a
at_clean <- at_clean %>%
  filter(
    !is.na(amt_annuity) &
    !is.na(obs_30_cnt_social_circle) &
    !is.na(def_30_cnt_social_circle) &
    !is.na(obs_60_cnt_social_circle) &
    !is.na(def_60_cnt_social_circle) &
    !is.na(days_last_phone_change)
  )


```

## Bureau clean up

The next step is to clean up and aggregate the bureau table to be able join it to the application train table to be able to some of the credit data as predictors. Some of the steps include:
- Aggregating to the same grain as application_train
- The sk_id_curr will be the joining column, so the data will we grouped by it, and then the data will aggregated to sums and counts based on the type of credit line.

``` {r}
bureau <- clean_names(bureau)
bureau$credit_active <- as.factor(bureau$credit_active)
bureau$credit_type <- as.factor(bureau$credit_type)
bureau <- clean_names(bureau)

bureau_agg <- bureau %>%
  group_by(sk_id_curr) %>%
  summarise(
    total_past_due = sum(amt_credit_sum_overdue, na.rm = TRUE),  # Sum of past due amounts
    number_of_accounts = n(),  # Count of rows per sk_id_curr
    number_of_paid_accounts = sum(credit_active == 'Closed' & (is.na(amt_credit_sum_debt) | amt_credit_sum_debt == 0)),  # Count of paid off accounts
    ct_mortgage_auto = sum(credit_type %in% c('Car loan', 'Mortgage', 'Real estate loan')),  # Count of mortgage and auto-related credit types
    ct_chargoff_accts = sum(credit_active == 'Closed' & amt_credit_sum_debt > 0, na.rm = TRUE),  # Count of charge-off accounts
    sum_chargoff_balance = sum(ifelse(credit_active == 'Closed' & amt_credit_sum_debt > 0, amt_credit_sum_debt, 0), na.rm = TRUE),  # Sum of charge-off balances
    ct_paid_mortgage_auto = sum(credit_active == 'Closed' & (is.na(amt_credit_sum_debt) | amt_credit_sum_debt == 0) & 
                                credit_type %in% c('Car loan', 'Mortgage', 'Real estate loan'))  # Count of paid off mortgage/auto-related accounts
  )


```


## Joining the tables

Here, I just joined the aggregated bureau table to the cleaned up application_train using a left join, and then removing all n/a's.

``` {r}

at_join <- at_clean %>%
  left_join(bureau_agg, by = "sk_id_curr")


# converting NA's to 0's
at_join <- at_join %>%
  mutate(
    total_past_due = replace_na(total_past_due, 0),
    number_of_accounts = replace_na(number_of_accounts, 0),
    number_of_paid_accounts = replace_na(number_of_paid_accounts, 0),
    ct_mortgage_auto = replace_na(ct_mortgage_auto, 0),
    ct_chargoff_accts = replace_na(ct_chargoff_accts, 0),
    sum_chargoff_balance = replace_na(sum_chargoff_balance, 0),
    ct_paid_mortgage_auto = replace_na(ct_paid_mortgage_auto, 0)
  )


```


## Additional clean up processes 

I also looked at all the flag_document variables. I looked at the counts and if they had any relationships with the target variable and found out that a lot of them we're missing, or that there was no relationship with the target variable, so removing all those variables should be beneficial.

Then I converted a few variables that still had negative values to positive.

And then decided to create an application_type (individual vs. co-applied) based on if the applicant was accompanied by someone.
``` {r}

#Remove all flag_document variables
at_join <- at_join %>%
  select(-starts_with("flag_document"))

# Convert negative values to postiive
at_join <- at_join %>%
  mutate(
    days_birth = abs(days_birth),
    days_registration = abs(days_registration),
    days_id_publish = abs(days_id_publish),
    days_last_phone_change = abs(days_last_phone_change)
  )

# Create application type based on name_type_suite
at_join <- at_join %>%
  mutate(application_type = factor(ifelse(name_type_suite == "Unaccompanied", "Individual", "Co-applied"))) %>%
  select(-name_type_suite, -organization_type)

```


# Modeling

## Setting up the data

First, I'm going to run a simple random forest to determine which variables are important, so I don't have to run the actual models on all of the data. To keep it simple and fast, I will only use 10% of the the data.

``` {r}
set.seed(123) 

# Sampling 10% of the data
sample_data <- at_join[sample(nrow(at_join), 0.1 * nrow(at_join)), ]
sample_data <- sample_data[, -1] 

# running a simple random forest on the sampled data
rf_model <- randomForest(target ~ ., data = sample_data, importance = TRUE)

# Ordering the top features by their importance, and selecting the top 20
importance_rf <- importance(rf_model)
top_features <- order(importance_rf[, "MeanDecreaseGini"], decreasing = TRUE)[1:20]

# Getting the names of the top 20 features, and only selecting them in at_join
top_feature_names <- names(sample_data)[top_features]
at_join_top20 <- at_join[, c(top_feature_names, "target")]

```

Next, I will sample the data down to 50% to reduce model training time. By using createDataPartitioning I can make sure that the the target variable keeps the original balance.

I also tried to run the model on balanced data (data where the target has the same amount of observations for both classes) but it didn't provide any improvement to the model (see the code that's been commented out)

Additionally, here in this code block I split the sampled data (the 50% of the original) into train and test sets using 80/20 split, and then since xgboost requires numeric variables, I created dummy variables for all categorical variables.

``` {r}
set.seed(123)

# Step 1: Stratified sampling
sample_index <- createDataPartition(at_join_top20$target, p = 0.5, list = FALSE)
sampled_data <- at_join_top20[sample_index, ]

# Step 2: Separate the classes
#class_0 <- sampled_data[sampled_data$target == 0, ]
#class_1 <- sampled_data[sampled_data$target == 1, ]

# Step 3: Downsample the majority class (0 class) to match the number of instances in class 1
#class_0_downsampled <- class_0[sample(nrow(class_0), nrow(class_1)), ]

# Step 4: Combine the balanced classes
#balanced_data <- rbind(class_0_downsampled, class_1)

# Step 5: Shuffle the combined dataset
#balanced_data <- balanced_data[sample(nrow(balanced_data)), ]

# Step 6: Train-test split on balanced data (80% train, 20% test)
trainIndex <- createDataPartition(sampled_data$target, p = 0.8, list = FALSE)
train_data <- sampled_data[trainIndex, ]
test_data <- sampled_data[-trainIndex, ]

# Step 7: Apply dummy encoding to the train and test data
dummy_vars <- dummyVars("~ .", data = train_data[, -ncol(train_data)])
train_data_transformed <- predict(dummy_vars, newdata = train_data)
test_data_transformed <- predict(dummy_vars, newdata = test_data)

```

## XGBoost model

Here, I set up the XGBoost model. I played around with many different hyperparameters but always ran into the same issue; the model got too complex and took hours to run, or in the end the model performance didn't match the expectations.

``` {r}

# Prepare labels
train_labels <- as.numeric(train_data$target) - 1  # Adjusted labels for XGBoost
test_labels <- as.numeric(test_data$target) - 1

# Step 8: Set up cross-validation and grid search for hyperparameter tuning
train_control <- trainControl(method = "cv", number = 3)  # 3-fold cross-validation

# Define a grid of hyperparameters to search
param_grid <- expand.grid(
  nrounds = 1500,  # Number of boosting iterations
  max_depth = 6, # Maximum depth of trees
  eta = 0.05,  # Learning rate
  gamma = 0, # Minimum loss reduction for a split
  colsample_bytree = 1, # Proportion of features to consider
  min_child_weight = 1,  # Minimum sum of instance weight (hessian) needed in a child
  subsample = 1  # Proportion of training data to randomly sample for each tree
)

# Step 9: Train the XGBoost model using caret
xgb_model_caret <- train(
  x = train_data_transformed,
  y = factor(train_labels), 
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = param_grid,
  verbose = TRUE 
)

#xgb_model_caret

# Step 10: Predict on the test data using the trained model
xgb_predictions <- predict(xgb_model_caret, newdata = test_data_transformed)
xgb_predictions_class <- as.numeric(xgb_predictions) - 1  # Convert factor back to numeric (0/1)

# Step 11: Evaluate performance metrics
conf_matrix <- confusionMatrix(as.factor(xgb_predictions_class), as.factor(test_data$target))

# Print the performance metrics
print(conf_matrix)

prop.table(table(at_join$target)) # majority class

```
As Can be seen in the confusion matrix, the accuracy of the model is only 0.9184. The model barely made any predictions of the negative class (in this case target = 1), so something went wrong in the training and the model couldn't accurately predict the minority class.

## XGBoost with manual predictor selection

Next, I'm going to select predictors based on my own experience. I work in lending and are familiar with our risk when it comes to application variables. However, it's good to know that I work in subprime lending, so those variables could be completely different.

For us, income, employment stability, and things like credit trade line history are good indicators of default  so I'll use mainly those as predictors.

``` {r}


# select specified features from the at_join dataset
selected_features <- at_join[, c(
  "flag_own_realty", 
  "amt_income_total", 
  "amt_credit", 
  "days_employed", 
  "name_education_type", 
  "avg_credit_score", 
  "full_credit_flag", 
  "no_credit_flag", 
  "total_past_due", 
  "number_of_accounts", 
  "number_of_paid_accounts", 
  "ct_mortgage_auto", 
  "ct_chargoff_accts", 
  "sum_chargoff_balance", 
  "ct_paid_mortgage_auto", 
  "application_type", 
  "target" 
)]

# split data 
set.seed(123) 
train_index <- createDataPartition(selected_features$target, p = 0.8, list = FALSE)
train_data_manual <- selected_features[train_index, ]
test_data_manual <- selected_features[-train_index, ]

# apply dummy encoding to categorical variables
dummy_vars_manual <- dummyVars("~ .", data = train_data_manual[, -ncol(train_data_manual)], fullRank = TRUE)
train_data_transformed_manual <- predict(dummy_vars_manual, newdata = train_data_manual)
test_data_transformed_manual <- predict(dummy_vars_manual, newdata = test_data_manual)

# prepare labels for XGBoost
train_labels_manual <- as.numeric(train_data_manual$target) - 1  # Adjusted labels for XGBoost
test_labels_manual <- as.numeric(test_data_manual$target) - 1

# set up cross-validation and parameter grid
train_control_manual <- trainControl(method = "cv", number = 3)  # 3-fold cross-validation

# set up xgboost params
param_grid_manual <- expand.grid(
  nrounds = 1500,         # Number of boosting iterations
  max_depth = 6,          # Maximum depth of trees
  eta = 0.05,             # Learning rate
  gamma = 0,              # Minimum loss reduction for a split
  colsample_bytree = 1,   # Proportion of features to consider
  min_child_weight = 1,   # Minimum sum of instance weight (hessian) needed in a child
  subsample = 1           # Proportion of training data to randomly sample for each tree
)

# add weights to the minority class 
class_ratio <- table(train_data_manual$target)
scale_pos_weight <- class_ratio[1] / class_ratio[2]

# Step 5: Train the XGBoost model using caret
xgb_model_caret_manual <- train(
  x = train_data_transformed_manual,
  y = factor(train_labels_manual),  # Ensure labels are factors for classification
  method = "xgbTree",
  trControl = train_control_manual,
  tuneGrid = param_grid_manual,
  verbose = TRUE,
  scale_pos_weight = scale_pos_weight
)

# predict on the test data
xgb_predictions_manual <- predict(xgb_model_caret_manual, newdata = test_data_transformed_manual)
xgb_predictions_class_manual <- as.numeric(xgb_predictions_manual) - 1  # Convert factor back to numeric (0/1)

# evaluate performance with a confusion matrix
conf_matrix_manual <- confusionMatrix(as.factor(xgb_predictions_class_manual), as.factor(test_data_manual$target))

print(conf_matrix_manual)


```

Again, the model didn't perform great. As we can see, the accuracy is finally at the same level as the majority class classifier, but as we can see in the confusion matrix, the model barely made any predictions of the negative class (target = 1), so basically the same thing as just using the majority class for prediction. Not great. I even tried adjusting the weights for the classes since they are so imbalanced, but it didn't help at all.

## XGBoost with balanced classes

Next, I'm going to try to see if balancing the classes will make the model performance any better. By balancing, I mean that I will keep all records of the minority class (rows where target = 1) and downsample the majority class (rows where target = 0) to have the same amount of records (24,719 rows each) and combine them into a balanced dataframe that will be used for modeling.

``` {r}

majority_class <- selected_features[selected_features$target == 0, ]
minority_class <- selected_features[selected_features$target == 1, ]

# undersampling the majority class to match the size of the minority class
set.seed(123)
majority_class_undersampled <- majority_class[sample(nrow(majority_class), nrow(minority_class)), ]

# combining and shuffling the dataset
balanced_data <- rbind(majority_class_undersampled, minority_class)
balanced_data <- balanced_data[sample(nrow(balanced_data)), ]

# train-test split (80% train, 20% test)
train_index_balanced <- createDataPartition(balanced_data$target, p = 0.8, list = FALSE)
train_data_balanced <- balanced_data[train_index_balanced, ]
test_data_balanced <- balanced_data[-train_index_balanced, ]

# apply dummy encoding to categorical variables
dummy_vars_balanced <- dummyVars("~ .", data = train_data_balanced[, -ncol(train_data_balanced)], fullRank = TRUE)
train_data_transformed_balanced <- predict(dummy_vars_balanced, newdata = train_data_balanced)
test_data_transformed_balanced <- predict(dummy_vars_balanced, newdata = test_data_balanced)

# prepare labels for XGBoost
train_labels_balanced <- as.numeric(train_data_balanced$target) - 1  # Adjusted labels for XGBoost
test_labels_balanced <- as.numeric(test_data_balanced$target) - 1

# set up cross-validation and parameter grid
train_control_balanced <- trainControl(method = "cv", number = 3)  # 3-fold cross-validation

# set up xgboost params
param_grid_balanced <- expand.grid(
  nrounds = 1500,         # Number of boosting iterations
  max_depth = 6,          # Maximum depth of trees
  eta = 0.05,             # Learning rate
  gamma = 0,              # Minimum loss reduction for a split
  colsample_bytree = 1,   # Proportion of features to consider
  min_child_weight = 1,   # Minimum sum of instance weight (hessian) needed in a child
  subsample = 1           # Proportion of training data to randomly sample for each tree
)

# train the XGBoost model
xgb_model_balanced <- train(
  x = train_data_transformed_balanced,
  y = factor(train_labels_balanced),  
  method = "xgbTree",
  trControl = train_control_balanced,
  tuneGrid = param_grid_balanced,
  verbose = TRUE
)

# predict on the test data
xgb_predictions_balanced <- predict(xgb_model_balanced, newdata = test_data_transformed_balanced)
xgb_predictions_class_balanced <- as.numeric(xgb_predictions_balanced) - 1  # Convert factor back to numeric (0/1)

# evaluate performance with a confusion matrix
conf_matrix_balanced <- confusionMatrix(as.factor(xgb_predictions_class_balanced), as.factor(test_data_balanced$target))

print(conf_matrix_balanced)


```


Again, pretty terrible performance. Accuracy is only 66.57%.

## AUC on all of the models

Next I'll plot the AUC curves to visualize the model performance for each XGBoost model

``` {r}

# Load necessary libraries
library(caret)
library(xgboost)
library(pROC)  # For ROC and AUC


# Calculate AUC and ROC curve for the initial XGBoost model
pred_probs_initial <- predict(xgb_model_caret, newdata = test_data_transformed, type = "prob")[, 2]
roc_initial <- roc(test_data$target, pred_probs_initial)
auc_initial <- auc(roc_initial)
print(paste("AUC for initial model:", auc_initial))

# Calculate AUC and ROC curve for the weighted model
pred_probs_manual <- predict(xgb_model_caret_manual, newdata = test_data_transformed_manual, type = "prob")[, 2]
roc_manual <- roc(test_data_manual$target, pred_probs_manual)
auc_manual <- auc(roc_manual)
print(paste("AUC for weighted model:", auc_manual))

# Calculate AUC and ROC curve for the undersampled model
pred_probs_balanced <- predict(xgb_model_balanced, newdata = test_data_transformed_balanced, type = "prob")[, 2]
roc_balanced <- roc(test_data_balanced$target, pred_probs_balanced)
auc_balanced <- auc(roc_balanced)
print(paste("AUC for undersampled model:", auc_balanced))

# Plot the ROC curves for comparison
plot(roc_initial, col = "green", main = "ROC Curves for XGBoost Models", print.auc = TRUE, print.auc.y = 0.5)
lines(roc_manual, col = "blue")
lines(roc_balanced, col = "red")
legend("bottomright", legend = c("Initial Model", "Weighted Model", "Undersampled Model"), col = c("green", "blue", "red"), lwd = 2)




```

As we can tell by the AUC curves, the models didn't perform that well as the AUC is only around 0.665. Better than random guessing, but still slightly worse or equal to using the majority class classifier.

# Predicting on Application Test

## Making the data match the transformed application_train data

``` {r}

# Factoring all character variables
a_clean <- application_test %>%
  mutate(across(where(is.character), as.factor))

#Factoring all 'flag' varaibles
a_clean <- a_clean %>%
  mutate(across(matches("flag", ignore.case = TRUE), as.factor))

# Factoring all binary numeric variables
a_clean <- a_clean %>%
  mutate(across(c(REG_REGION_NOT_LIVE_REGION,
                  REG_REGION_NOT_WORK_REGION,
                  LIVE_REGION_NOT_WORK_REGION,
                  REG_CITY_NOT_LIVE_CITY,
                  REG_CITY_NOT_WORK_CITY,
                  LIVE_CITY_NOT_WORK_CITY), as.factor))

# Converting column names to lowercase
a_clean <- a_clean %>% clean_names()

# Defining all living situation variables that are unnecessary for modeling
living_situation_vars <- c(
  "apartments_avg", "basementarea_avg", "years_beginexpluatation_avg", 
  "years_build_avg", "commonarea_avg", "elevators_avg", 
  "entrances_avg", "floorsmax_avg", "floorsmin_avg", 
  "landarea_avg", "livingapartments_avg", "livingarea_avg", 
  "nonlivingapartments_avg", "nonlivingarea_avg", "apartments_mode", 
  "basementarea_mode", "years_beginexpluatation_mode", "years_build_mode", 
  "commonarea_mode", "elevators_mode", "entrances_mode", 
  "floorsmax_mode", "floorsmin_mode", "landarea_mode", 
  "livingapartments_mode", "livingarea_mode", "nonlivingapartments_mode", 
  "nonlivingarea_mode", "apartments_medi", "basementarea_medi", 
  "years_beginexpluatation_medi", "years_build_medi", "commonarea_medi", 
  "elevators_medi", "entrances_medi", "floorsmax_medi", 
  "floorsmin_medi", "landarea_medi", "livingapartments_medi", 
  "livingarea_medi", "nonlivingapartments_medi", "nonlivingarea_medi",  "totalarea_mode"
)

# Removing all living situation variables and a few others not defined before
a_clean <- a_clean %>%
  select(-all_of(living_situation_vars), 
         -fondkapremont_mode, 
         -housetype_mode, 
         -wallsmaterial_mode, 
         -emergencystate_mode)

# Fixing the issues with days employed variable
a_clean <- a_clean %>%
  mutate(days_employed = ifelse(days_employed > 0, 0, days_employed))

a_clean <- a_clean %>%
  mutate(days_employed = abs(days_employed))

# Simplifying the Occupation type variable
a_clean <- a_clean %>%
  mutate(occupation_type = case_when(
    is.na(occupation_type) & days_employed > 0 ~ 'Not listed',
    is.na(occupation_type) & days_employed == 0 ~ 'Unemployed',
    TRUE ~ occupation_type  # Keep original value if not NA
  )) %>% mutate(occupation_type = factor(occupation_type))

# Removing all rows where name_type_suite is n/a
a_clean <- a_clean %>%
  mutate(name_type_suite = replace_na(name_type_suite, "Unaccompanied"))

# Combining credit scores to an average credit score
a_clean <- a_clean %>%
  mutate(avg_credit_score = rowMeans(
    select(., ext_source_1, ext_source_2, ext_source_3), 
    na.rm = TRUE
  )) %>% 
  mutate(avg_credit_score = ifelse(is.na(avg_credit_score), 0, avg_credit_score))

# Creating credit flags based on how many bureau credit scores are available
a_clean <- a_clean %>%
  mutate(
    limited_credit_flag = case_when(
      rowSums(!is.na(select(., ext_source_1, ext_source_2, ext_source_3))) %in% 1:2 ~ 1,
      TRUE ~ 0
    ),
    no_credit_flag = case_when(
      rowSums(is.na(select(., ext_source_1, ext_source_2, ext_source_3))) == 3 ~ 1,
      TRUE ~ 0
    ),
    full_credit_flag = case_when(
      rowSums(!is.na(select(., ext_source_1, ext_source_2, ext_source_3))) == 3 ~ 1,
      TRUE ~ 0
    )
  ) %>%
  mutate(
    limited_credit_flag = factor(limited_credit_flag),
    no_credit_flag = factor(no_credit_flag),
    full_credit_flag = factor(full_credit_flag)
  ) %>%
  select(-ext_source_1, -ext_source_2, -ext_source_3)

# Simplifying the own car age variable 
a_clean <- a_clean %>%
  mutate(own_car_age = case_when(
    is.na(own_car_age) ~ 'No car',
    own_car_age >= 10 ~ '10+ years',
    own_car_age < 10 ~ 'Less than 10 years'
  )) %>%
  mutate(own_car_age = as.factor(own_car_age))

# Replacing n/a's with 0
a_clean <- a_clean %>%
  mutate(
    amt_req_credit_bureau_hour = replace_na(amt_req_credit_bureau_hour, 0),
    amt_req_credit_bureau_day = replace_na(amt_req_credit_bureau_day, 0),
    amt_req_credit_bureau_week = replace_na(amt_req_credit_bureau_week, 0),
    amt_req_credit_bureau_mon = replace_na(amt_req_credit_bureau_mon, 0),
    amt_req_credit_bureau_qrt = replace_na(amt_req_credit_bureau_qrt, 0),
    amt_req_credit_bureau_year = replace_na(amt_req_credit_bureau_year, 0)
  )

# Removing all rows where the following variables are n/a
a_clean <- a_clean %>%
  mutate(
    amt_annuity = replace_na(amt_annuity, 0),
    obs_30_cnt_social_circle = replace_na(obs_30_cnt_social_circle, 0),
    def_30_cnt_social_circle = replace_na(def_30_cnt_social_circle, 0),
    obs_60_cnt_social_circle = replace_na(obs_60_cnt_social_circle, 0),
    def_60_cnt_social_circle = replace_na(def_60_cnt_social_circle, 0),
    days_last_phone_change = replace_na(days_last_phone_change, 0)
  )


a_join <- a_clean %>%
  left_join(bureau_agg, by = "sk_id_curr")


# converting NA's to 0's
a_join <- a_join %>%
  mutate(
    total_past_due = replace_na(total_past_due, 0),
    number_of_accounts = replace_na(number_of_accounts, 0),
    number_of_paid_accounts = replace_na(number_of_paid_accounts, 0),
    ct_mortgage_auto = replace_na(ct_mortgage_auto, 0),
    ct_chargoff_accts = replace_na(ct_chargoff_accts, 0),
    sum_chargoff_balance = replace_na(sum_chargoff_balance, 0),
    ct_paid_mortgage_auto = replace_na(ct_paid_mortgage_auto, 0)
  )

a_join <- a_join %>%
  select(-starts_with("flag_document"))

# Convert negative values to postiive
a_join <- a_join %>%
  mutate(
    days_birth = abs(days_birth),
    days_registration = abs(days_registration),
    days_id_publish = abs(days_id_publish),
    days_last_phone_change = abs(days_last_phone_change)
  )

# Create application type based on name_type_suite
a_join <- a_join %>%
  mutate(application_type = factor(ifelse(name_type_suite == "Unaccompanied", "Individual", "Co-applied"))) %>%
  select(-name_type_suite, -organization_type)


a_join_top20 <- a_join[, top_feature_names]

```

## Predicting the first XGBoost model 

``` {r}
a_join_top20_transformed <- predict(dummy_vars, newdata = a_join_top20)

# Step 2: Predict on the transformed actual test data
a_join_predictions <- predict(xgb_model_caret, newdata = a_join_top20_transformed)
a_join_predictions_class <- as.numeric(a_join_predictions) - 1  # Convert factor back to numeric (0/1)

# Step 3: Add the predictions as the 21st column in a_join_top20
a_join_top20$prediction <- a_join_predictions_class

table(a_join_top20$prediction)

```

Similar to what we saw in the training phase, the model barely made any predictions of the minority class.

## Predicting the second XGBoost model

``` {r}
# Step 1: Select the manually chosen features in a_join (excluding 'target' since a_join is actual test data)
selected_features_a_join <- a_join[, c(
  "flag_own_realty", 
  "amt_income_total", 
  "amt_credit", 
  "days_employed", 
  "name_education_type", 
  "avg_credit_score", 
  "full_credit_flag", 
  "no_credit_flag", 
  "total_past_due", 
  "number_of_accounts", 
  "number_of_paid_accounts", 
  "ct_mortgage_auto", 
  "ct_chargoff_accts", 
  "sum_chargoff_balance", 
  "ct_paid_mortgage_auto", 
  "application_type"
)]

# Step 2: Apply dummy encoding to a_join's selected features
a_join_transformed_manual <- predict(dummy_vars_manual, newdata = selected_features_a_join)

# Step 3: Predict on the transformed a_join data
a_join_predictions_manual <- predict(xgb_model_caret_manual, newdata = a_join_transformed_manual)
a_join_predictions_class_manual <- as.numeric(a_join_predictions_manual) - 1  # Convert factor back to numeric (0/1)

# Step 4: Add predictions as a new column in a_join
a_join$prediction <- a_join_predictions_class_manual

table(a_join$prediction)


```

Similar to what we saw in the training phase, the model barely made any predictions of the minority class.

## Predicting the third XGboost model

``` {r}
a_join_transformed_balanced <- predict(dummy_vars_balanced, newdata = selected_features_a_join)

# Step 2: Predict on the transformed a_join data using the balanced model
a_join_predictions_balanced <- predict(xgb_model_balanced, newdata = a_join_transformed_balanced)
a_join_predictions_class_balanced <- as.numeric(a_join_predictions_balanced) - 1

a_join_balanced_predictions <- selected_features_a_join

# Add the new prediction column
a_join_balanced_predictions$prediction_balanced <- a_join_predictions_class_balanced

table(a_join_balanced_predictions$prediction_balanced)
```
## Storing application_test predictions in single-column data frames

This just creates three single-column dataframes with the predictions for application_test. This data is in format that should be able to be submitted to the Kaggle competition if needed.

``` {r}
application_test_predictions1 <- a_join_top20 %>%
  select(prediction)

application_test_predictions2 <- a_join %>%
  select(prediction) 

application_test_predictions3 <- a_join_balanced_predictions %>%
  select(prediction_balanced)


```

# Conclusions and final thoughts:


So far, I haven't been able to get the XGBoost models to outperform the majority class in prediction. There could be many reasons for this: it could be the hyperparameters of the models, the predictors being used, or the approach to adjusting the class imbalances, which may not have been optimal.

**Hyperparamaters:** For each of the models, I used a high number of boosts (trees). I thought the data was fairly complex, so I wanted to ensure that the model captures all potential relationships, if any. Similarly, with max_depth, I aimed to capture deeper relationships in the data. By setting ETA (the learning rate) low, I aimed to prevent overfitting, which might occur with a higher number of boosts. Additionally, I set gamma to 0 to maintain the model’s complexity. Lastly, by setting colsample_bytree, min_child_weight, and subsample to 1, I ensured that all features were considered in each model.

**Variables:** As I explained in the EDA notebook, most of my decisions stem from my experience in lending (specifically subprime auto lending). Some variables and considerations may differ in Home Credit’s scenario, so exploring all variables more thoroughly might help identify predictors that are a better fit for this context. Another consideration is exploring all available data. So far, I’ve only joined the bureau data with the main table (application_train). Additional data includes more information on applicants’ past payment history, credit card balances, and previous applications with Home Credit, which could provide valuable insights.

**Class Imbalances:** I experimented with handling the class imbalance in a few ways: using the original imbalanced data (where about 8% belongs to the minority class, target = 1) and using a balanced, resampled dataset with a 50/50 class split. However, a 50/50 split isn't always necessary for balancing the target variable; exploring other ratios, like 30/70, could be beneficial moving forward.

**Cross-validation:**Each of the trained models included some level of cross-validation, as I used caret’s train function to run each model, defining cross-validation within the train control. However, there’s always room for more cross-validation! In models where I used sampling to prepare the data for training, I could add an additional layer of cross-validation to the sampling itself, rather than just cross-validating within the model.

**Final conclusion:** I wasn't able to achieve the accuracy level I hoped for with the XGBoost models I trained. XGBoost is a powerful machine learning technique, capable of handling complex datasets like this one. There could be multiple reasons why I didn’t reach the desired accuracy level. Due to my limited knowledge of XGBoost, I wasn’t able to pinpoint the primary reasons.

**Next Steps:** Honestly, I think the next step would be to go back to EDA. As I explained, I think there's much more to discover in the data to get it prepared for modeling. Joining additional tables, exploring different variables, and doing modifications to make the data a better fit for modeling would be a good place to start.


